{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejhaselden/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "34.18670296669006\n",
      "4469395.5\n"
     ]
    }
   ],
   "source": [
    "## Update this later with tuned parameters\n",
    "\n",
    "#get embedding model and compute loss (for use in hyperparameter tuning)\n",
    "import w2v_for_lm\n",
    "import time\n",
    "\n",
    "for i in range(1,2):\n",
    "    print(i)\n",
    "    start = time.time()\n",
    "    epochs = i\n",
    "    vec_size = 10\n",
    "    window = 5\n",
    "    model = w2v_for_lm.retrieve_model_na(epochs, vec_size, window)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    loss = model.get_latest_training_loss()\n",
    "    # perplexity = 2**loss\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = w2v_for_lm.retrieve_query()\n",
    "wordlist = test['cn'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys, time\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "\n",
    "# NLTK is the Natural Language Toolkit, and contains several language datasets\n",
    "# as well as implementations of many popular NLP algorithms.\n",
    "# HINT: You should look at what is available here when thinking about your project!\n",
    "import nltk\n",
    "\n",
    "# Helper libraries (see the corresponding py files in this notebook's directory).\n",
    "import utils, vocabulary\n",
    "\n",
    "# import segment\n",
    "\n",
    "utils.require_package(\"tqdm\")  # for nice progress bars\n",
    "from tqdm import tqdm as ProgressBar\n",
    "\n",
    "# # Bokeh for plotting.\n",
    "utils.require_package(\"bokeh\")\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics\n",
    "\n",
    "NLTK includes a number of corpora that we can experiment with for this exercise. Different types of text can have very different N-gram distributions, and some are more difficult to model than others.\n",
    "\n",
    "Let's start with the [Brown corpus](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html), the first major computer-readable linguistic corpus. It consists of around 1 million words of American English, sampled from 15 different text categories ranging from news text to academic articles to popular fiction.\n",
    "\n",
    "If you haven't yet run `nltk.download()`, the cell below will download the Brown corpus for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the all the words in the corpus, and looking at some basic statistics.\n",
    "\n",
    "We've built a helper class, `Vocabulary`, that ingests a list of words, counts their frequencies, and assigns each one a numerical ID that will be useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_corpus = model.wv.index_to_key\n",
    "# corpus.words()\n",
    "\n",
    "# embedded_corpus.remove(None)\n",
    "# embedded_corpus.count(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOT FROM HERE\n",
    "import re\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "779402it [00:16, 46206.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 735,870\n"
     ]
    }
   ],
   "source": [
    "# corpus = nltk.corpus.brown\n",
    "# corpus = embedded_corpus\n",
    "\n",
    "# \"canonicalize_word\" performs a few tweaks to the token stream of\n",
    "# the corpus.  For example, it replaces digits with DG allowing numbers\n",
    "# to aggregate together when we count them below.\n",
    "# You can read the details in utils.py if you're really curious.\n",
    "\n",
    "\n",
    "# token_feed = (utils.canonicalize_word(w) for w in corpus.words())\n",
    "token_feed = (canonicalize_word(w) for w in wordlist)\n",
    "\n",
    "\n",
    "\n",
    "# # Collect counts of tokens and assign wordids.\n",
    "vocab = vocabulary.Vocabulary(token_feed, progressbar=ProgressBar)\n",
    "# vocab = vocabulary.Vocabulary(token_feed)\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "\n",
    "# # Print out some (debugging) statistics to make sure everything went\n",
    "# # as we expected.  (Unsurprisingly, you should see \"the\" as the most popular word.)\n",
    "# print(\"Most common unigrams:\")\n",
    "# for word, count in vocab.unigram_counts.most_common(10):\n",
    "#     print(\"\\\"{:s}\\\": {:,}\".format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"a0f5d75c-bfd3-44e0-ad04-82dd1545d70b\" data-root-id=\"1003\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"711a8a81-4eda-4fe9-b78b-1f4a5b6074e4\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\"}],\"center\":[{\"id\":\"1014\"},{\"id\":\"1018\"}],\"left\":[{\"id\":\"1015\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1025\"}],\"title\":{\"id\":\"1027\"},\"toolbar\":{\"id\":\"1019\"},\"x_range\":{\"id\":\"1004\"},\"x_scale\":{\"id\":\"1008\"},\"y_range\":{\"id\":\"1006\"},\"y_scale\":{\"id\":\"1010\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis\":{\"id\":\"1012\"},\"ticker\":null},\"id\":\"1014\",\"type\":\"Grid\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1002\"}]},\"id\":\"1019\",\"type\":\"Toolbar\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1027\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"1021\"},\"glyph\":{\"id\":\"1022\"},\"hover_glyph\":{\"id\":\"1024\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1023\"},\"selection_glyph\":null,\"view\":{\"id\":\"1026\"}},\"id\":\"1025\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1032\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1023\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1030\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1016\",\"type\":\"BasicTicker\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1022\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1033\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1034\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"factors\":[\"product\",\"masterlist\",\"text, font\",\"775571579, bffulltakes_ftp\",\"font\",\"bffulltakes_ftp\",\"775581421, bffulltakes_ftp, arts culture and entertainment\",\"text\",\"775571578, bffulltakes_ftp\",\"DGDGDGDGDGDGDGDGDG\",\"line\",\"775575301, 20201014_billboard music awards_nbc, bffulltakes_ftp\",\"bffulltakes_ftp, 775571579\",\"kpphotoshoot\",\"font, text\",\"20201014_billboard music awards_nbc, 775575301, bffulltakes_ftp\",\"775581420, arts culture and entertainment\",\"line, text, font\",\"text, logo, font\",\"text, line, font\"]},\"id\":\"1004\",\"type\":\"FactorRange\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1024\",\"type\":\"VBar\"},{\"attributes\":{\"start\":0},\"id\":\"1006\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"1002\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"1032\"},\"ticker\":{\"id\":\"1016\"}},\"id\":\"1015\",\"type\":\"LinearAxis\"},{\"attributes\":{\"axis\":{\"id\":\"1015\"},\"dimension\":1,\"ticker\":null},\"id\":\"1018\",\"type\":\"Grid\"},{\"attributes\":{\"data\":{\"top\":[768,732,563,482,464,332,290,263,254,237,224,219,213,206,196,195,171,165,160,157],\"x\":[\"product\",\"masterlist\",\"text, font\",\"775571579, bffulltakes_ftp\",\"font\",\"bffulltakes_ftp\",\"775581421, bffulltakes_ftp, arts culture and entertainment\",\"text\",\"775571578, bffulltakes_ftp\",\"DGDGDGDGDGDGDGDGDG\",\"line\",\"775575301, 20201014_billboard music awards_nbc, bffulltakes_ftp\",\"bffulltakes_ftp, 775571579\",\"kpphotoshoot\",\"font, text\",\"20201014_billboard music awards_nbc, 775575301, bffulltakes_ftp\",\"775581420, arts culture and entertainment\",\"line, text, font\",\"text, logo, font\",\"text, line, font\"]},\"selected\":{\"id\":\"1033\"},\"selection_policy\":{\"id\":\"1034\"}},\"id\":\"1021\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"source\":{\"id\":\"1021\"}},\"id\":\"1026\",\"type\":\"CDSView\"},{\"attributes\":{\"formatter\":{\"id\":\"1030\"},\"ticker\":{\"id\":\"1013\"}},\"id\":\"1012\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"CategoricalTicker\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\n",
       "  var render_items = [{\"docid\":\"711a8a81-4eda-4fe9-b78b-1f4a5b6074e4\",\"root_ids\":[\"1003\"],\"roots\":{\"1003\":\"a0f5d75c-bfd3-44e0-ad04-82dd1545d70b\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1003"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words, counts = zip(*vocab.unigram_counts.most_common(20))\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], mode=\"vline\")\n",
    "fig = bp.figure(x_range=words, plot_width=800, plot_height=400, tools=[hover])\n",
    "fig.vbar(x=words, width=0.8, top=counts, hover_fill_color=\"firebrick\")\n",
    "fig.y_range.start = 0\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that it falls off very quickly! It also flattens out a lot after the initial dip.  Recall that word frequencies tend to follow **Zipf's law**, in that they are rougly proportional to $\\frac{1}{\\mathrm{rank}(w)}$, where rank = 1 for the most common word, 2 for the second-most, etc. We can test this directly with a numerical fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-125724f6a596>:15: RuntimeWarning: invalid value encountered in log\n",
      "  fit_func = lambda c: (np.log(p) - np.log(c[0] * rank**c[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power law exponent: Î² = -0.08\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"126dbcb0-833c-46c1-a52b-d29a864cc8cf\" data-root-id=\"1075\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"3f24b1e8-6a62-4b77-ac27-59b7a1941ff1\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1084\"}],\"center\":[{\"id\":\"1086\"},{\"id\":\"1090\"}],\"left\":[{\"id\":\"1087\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1109\"},{\"id\":\"1116\"}],\"title\":{\"id\":\"1126\"},\"toolbar\":{\"id\":\"1098\"},\"x_range\":{\"id\":\"1076\"},\"x_scale\":{\"id\":\"1080\"},\"y_range\":{\"id\":\"1078\"},\"y_scale\":{\"id\":\"1082\"}},\"id\":\"1075\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"source\":{\"id\":\"1105\"}},\"id\":\"1110\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1106\",\"type\":\"VBar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1107\",\"type\":\"VBar\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1108\",\"type\":\"VBar\"},{\"attributes\":{\"data_source\":{\"id\":\"1105\"},\"glyph\":{\"id\":\"1106\"},\"hover_glyph\":{\"id\":\"1108\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1107\"},\"selection_glyph\":null,\"view\":{\"id\":\"1110\"}},\"id\":\"1109\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"1113\"}},\"id\":\"1117\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1132\",\"type\":\"Selection\"},{\"attributes\":{\"data\":{\"x\":[\"product\",\"masterlist\",\"text, font\",\"775571579, bffulltakes_ftp\",\"font\",\"bffulltakes_ftp\",\"775581421, bffulltakes_ftp, arts culture and entertainment\",\"text\",\"775571578, bffulltakes_ftp\",\"DGDGDGDGDGDGDGDGDG\",\"line\",\"775575301, 20201014_billboard music awards_nbc, bffulltakes_ftp\",\"bffulltakes_ftp, 775571579\",\"kpphotoshoot\",\"font, text\",\"20201014_billboard music awards_nbc, 775575301, bffulltakes_ftp\",\"775581420, arts culture and entertainment\",\"line, text, font\",\"text, logo, font\",\"text, line, font\",\"775574058, bffulltakes_ftp, arts culture and entertainment\",\"masterlist, product\",\"logo, text, font\",\"logo\",\"enterprise\",\"rooster tail original\",\"arts culture and entertainment, 775581421, bffulltakes_ftp\",\"775576796, bffulltakes_ftp, arts culture and entertainment\",\"775571574, bffulltakes_ftp\",\"bffulltakes_ftp, 775571578\",\"775575292, bffulltakes_ftp\",\"lifestyle imagery\",\"775590358, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, arts culture and entertainment, 775581421\",\"bffulltakes_ftp, 775581421, arts culture and entertainment\",\"csic 2020\",\"lil corky\",\"yellow\",\"logistics\",\"winkelmeyer_1800 tequila x ashley greene full take_10.26.20_515 images, bffulltakes_ftp\",\"2020macysparade, 775581421, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, 20201014_billboard music awards_nbc, 775575301\",\"775571015, bffulltakes_ftp, arts culture and entertainment\",\"775581421, 2020macysparade, bffulltakes_ftp, arts culture and entertainment\",\"775588385, bffulltakes_ftp, arts culture and entertainment\",\"bfselects_ftp\",\"2020cmtarrivalsfulltake, bffulltakes_ftp\",\"775583653, bffulltakes_ftp, arts culture and entertainment\",\"rectangle, font\",\"DGDGDGDG\",\"spin-n-glo silver wing\",\"rooster tail vibric\",\"startup\",\"technology\",\"775587761, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, 775575301, 20201014_billboard music awards_nbc\",\"775583459, bffulltakes_ftp, arts culture and entertainment\",\"masterlist, tool\",\"logo, font, text\",\"office\",\"spin-n-glo white wing\",\"arts culture and entertainment, 775581420\",\"green\",\"775581421, arts culture and entertainment, bffulltakes_ftp\",\"graphic designer tools\",\"775571579, performance, bffulltakes_ftp\",\"775576796, event, bffulltakes_ftp, arts culture and entertainment\",\"intercontinental san juan resort\",\"775641564, music, fashion, celebrities, awards ceremony, arts culture and entertainment\",\"tool\",\"bffulltakes_ftp, 775574058, arts culture and entertainment\",\"4.1.1 afterpay, font\",\"arts culture and entertainment, 775574058, bffulltakes_ftp\",\"2.1.1 shop now pay later, font\",\"bffulltakes_ftp, 775571574\",\"text, font, line\",\"4.1.2 clearpay, font\",\"bffulltakes_ftp, arts culture and entertainment, 775574058\",\"775571527, bffulltakes_ftp, arts culture and entertainment\",\"775577363, bffulltakes_ftp, arts culture and entertainment\",\"775563042, bffulltakes_ftp\",\"775571579, red, bffulltakes_ftp\",\"line, font, text\",\"drink\",\"bffulltakes_ftp, 775575292\",\"text, font, logo\",\"775581420, red, arts culture and entertainment\",\"bffulltakes_ftp1, 775590397, arts culture and entertainment\",\"20201017_775578202_feedyourcitychallengechicago_516, bffulltakes_ftp\",\"775581418, bffulltakes_ftp, arts culture and entertainment\",\"composite material\",\"775571574, performance, bffulltakes_ftp\",\"bffulltakes_ftp, arts culture and entertainment, 775576796\",\"film\",\"logo, font\",\"arts culture and entertainment, 775590358, bffulltakes_ftp\",\"hilton barbados resort\",\"775578207, bffulltakes_ftp, arts culture and entertainment\",\"20201029_casamigoshalloween_775580960_kovac_488images, bffulltakes_ftp\",\"performance, 775571579, bffulltakes_ftp\"],\"y\":{\"__ndarray__\":\"AAAAAAAACEAAAAAAAAAIQAAAAAAAAAhAAAAAAAAACEAAAAAAAAAIQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[100]}},\"selected\":{\"id\":\"1134\"},\"selection_policy\":{\"id\":\"1135\"}},\"id\":\"1113\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1126\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1096\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1133\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"factors\":[\"product\",\"masterlist\",\"text, font\",\"775571579, bffulltakes_ftp\",\"font\",\"bffulltakes_ftp\",\"775581421, bffulltakes_ftp, arts culture and entertainment\",\"text\",\"775571578, bffulltakes_ftp\",\"DGDGDGDGDGDGDGDGDG\",\"line\",\"775575301, 20201014_billboard music awards_nbc, bffulltakes_ftp\",\"bffulltakes_ftp, 775571579\",\"kpphotoshoot\",\"font, text\",\"20201014_billboard music awards_nbc, 775575301, bffulltakes_ftp\",\"775581420, arts culture and entertainment\",\"line, text, font\",\"text, logo, font\",\"text, line, font\",\"775574058, bffulltakes_ftp, arts culture and entertainment\",\"masterlist, product\",\"logo, text, font\",\"logo\",\"enterprise\",\"rooster tail original\",\"arts culture and entertainment, 775581421, bffulltakes_ftp\",\"775576796, bffulltakes_ftp, arts culture and entertainment\",\"775571574, bffulltakes_ftp\",\"bffulltakes_ftp, 775571578\",\"775575292, bffulltakes_ftp\",\"lifestyle imagery\",\"775590358, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, arts culture and entertainment, 775581421\",\"bffulltakes_ftp, 775581421, arts culture and entertainment\",\"csic 2020\",\"lil corky\",\"yellow\",\"logistics\",\"winkelmeyer_1800 tequila x ashley greene full take_10.26.20_515 images, bffulltakes_ftp\",\"2020macysparade, 775581421, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, 20201014_billboard music awards_nbc, 775575301\",\"775571015, bffulltakes_ftp, arts culture and entertainment\",\"775581421, 2020macysparade, bffulltakes_ftp, arts culture and entertainment\",\"775588385, bffulltakes_ftp, arts culture and entertainment\",\"bfselects_ftp\",\"2020cmtarrivalsfulltake, bffulltakes_ftp\",\"775583653, bffulltakes_ftp, arts culture and entertainment\",\"rectangle, font\",\"DGDGDGDG\",\"spin-n-glo silver wing\",\"rooster tail vibric\",\"startup\",\"technology\",\"775587761, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, 775575301, 20201014_billboard music awards_nbc\",\"775583459, bffulltakes_ftp, arts culture and entertainment\",\"masterlist, tool\",\"logo, font, text\",\"office\",\"spin-n-glo white wing\",\"arts culture and entertainment, 775581420\",\"green\",\"775581421, arts culture and entertainment, bffulltakes_ftp\",\"graphic designer tools\",\"775571579, performance, bffulltakes_ftp\",\"775576796, event, bffulltakes_ftp, arts culture and entertainment\",\"intercontinental san juan resort\",\"775641564, music, fashion, celebrities, awards ceremony, arts culture and entertainment\",\"tool\",\"bffulltakes_ftp, 775574058, arts culture and entertainment\",\"4.1.1 afterpay, font\",\"arts culture and entertainment, 775574058, bffulltakes_ftp\",\"2.1.1 shop now pay later, font\",\"bffulltakes_ftp, 775571574\",\"text, font, line\",\"4.1.2 clearpay, font\",\"bffulltakes_ftp, arts culture and entertainment, 775574058\",\"775571527, bffulltakes_ftp, arts culture and entertainment\",\"775577363, bffulltakes_ftp, arts culture and entertainment\",\"775563042, bffulltakes_ftp\",\"775571579, red, bffulltakes_ftp\",\"line, font, text\",\"drink\",\"bffulltakes_ftp, 775575292\",\"text, font, logo\",\"775581420, red, arts culture and entertainment\",\"bffulltakes_ftp1, 775590397, arts culture and entertainment\",\"20201017_775578202_feedyourcitychallengechicago_516, bffulltakes_ftp\",\"775581418, bffulltakes_ftp, arts culture and entertainment\",\"composite material\",\"775571574, performance, bffulltakes_ftp\",\"bffulltakes_ftp, arts culture and entertainment, 775576796\",\"film\",\"logo, font\",\"arts culture and entertainment, 775590358, bffulltakes_ftp\",\"hilton barbados resort\",\"775578207, bffulltakes_ftp, arts culture and entertainment\",\"20201029_casamigoshalloween_775580960_kovac_488images, bffulltakes_ftp\",\"performance, 775571579, bffulltakes_ftp\"]},\"id\":\"1076\",\"type\":\"FactorRange\"},{\"attributes\":{\"line_color\":\"Orange\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1114\",\"type\":\"Line\"},{\"attributes\":{\"data\":{\"top\":{\"__ndarray__\":\"AAAAAAAAiEAAAAAAAOCGQAAAAAAAmIFAAAAAAAAgfkAAAAAAAAB9QAAAAAAAwHRAAAAAAAAgckAAAAAAAHBwQAAAAAAAwG9AAAAAAACgbUAAAAAAAABsQAAAAAAAYGtAAAAAAACgakAAAAAAAMBpQAAAAAAAgGhAAAAAAABgaEAAAAAAAGBlQAAAAAAAoGRAAAAAAAAAZEAAAAAAAKBjQAAAAAAAAGNAAAAAAADgYkAAAAAAACBiQAAAAAAAAGFAAAAAAADgYEAAAAAAAGBgQAAAAAAAQF9AAAAAAAAAXkAAAAAAAMBdQAAAAAAAQFtAAAAAAAAAW0AAAAAAAABbQAAAAAAAwFlAAAAAAADAWUAAAAAAAABZQAAAAAAAgFhAAAAAAABAWEAAAAAAAABYQAAAAAAAgFdAAAAAAADAVUAAAAAAAMBVQAAAAAAAgFVAAAAAAAAAVUAAAAAAAIBUQAAAAAAAwFNAAAAAAAAAU0AAAAAAAABTQAAAAAAAwFJAAAAAAADAUkAAAAAAAEBSQAAAAAAAQFJAAAAAAABAUkAAAAAAAABSQAAAAAAAAFJAAAAAAADAUUAAAAAAAMBRQAAAAAAAQFFAAAAAAAAAUUAAAAAAAABRQAAAAAAAwFBAAAAAAADAUEAAAAAAAMBQQAAAAAAAwFBAAAAAAABAUEAAAAAAAEBQQAAAAAAAAFBAAAAAAACAT0AAAAAAAIBPQAAAAAAAgE9AAAAAAACAT0AAAAAAAIBPQAAAAAAAAE9AAAAAAAAAT0AAAAAAAIBOQAAAAAAAAE5AAAAAAAAATkAAAAAAAABNQAAAAAAAAE1AAAAAAACATEAAAAAAAIBMQAAAAAAAAExAAAAAAACAS0AAAAAAAIBLQAAAAAAAgEpAAAAAAAAASkAAAAAAAABKQAAAAAAAgElAAAAAAACASUAAAAAAAIBIQAAAAAAAAEhAAAAAAAAASEAAAAAAAIBHQAAAAAAAgEdAAAAAAACAR0AAAAAAAIBHQAAAAAAAAEdAAAAAAACARkAAAAAAAIBGQAAAAAAAgEZAAAAAAACARkA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[100]},\"x\":[\"product\",\"masterlist\",\"text, font\",\"775571579, bffulltakes_ftp\",\"font\",\"bffulltakes_ftp\",\"775581421, bffulltakes_ftp, arts culture and entertainment\",\"text\",\"775571578, bffulltakes_ftp\",\"DGDGDGDGDGDGDGDGDG\",\"line\",\"775575301, 20201014_billboard music awards_nbc, bffulltakes_ftp\",\"bffulltakes_ftp, 775571579\",\"kpphotoshoot\",\"font, text\",\"20201014_billboard music awards_nbc, 775575301, bffulltakes_ftp\",\"775581420, arts culture and entertainment\",\"line, text, font\",\"text, logo, font\",\"text, line, font\",\"775574058, bffulltakes_ftp, arts culture and entertainment\",\"masterlist, product\",\"logo, text, font\",\"logo\",\"enterprise\",\"rooster tail original\",\"arts culture and entertainment, 775581421, bffulltakes_ftp\",\"775576796, bffulltakes_ftp, arts culture and entertainment\",\"775571574, bffulltakes_ftp\",\"bffulltakes_ftp, 775571578\",\"775575292, bffulltakes_ftp\",\"lifestyle imagery\",\"775590358, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, arts culture and entertainment, 775581421\",\"bffulltakes_ftp, 775581421, arts culture and entertainment\",\"csic 2020\",\"lil corky\",\"yellow\",\"logistics\",\"winkelmeyer_1800 tequila x ashley greene full take_10.26.20_515 images, bffulltakes_ftp\",\"2020macysparade, 775581421, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, 20201014_billboard music awards_nbc, 775575301\",\"775571015, bffulltakes_ftp, arts culture and entertainment\",\"775581421, 2020macysparade, bffulltakes_ftp, arts culture and entertainment\",\"775588385, bffulltakes_ftp, arts culture and entertainment\",\"bfselects_ftp\",\"2020cmtarrivalsfulltake, bffulltakes_ftp\",\"775583653, bffulltakes_ftp, arts culture and entertainment\",\"rectangle, font\",\"DGDGDGDG\",\"spin-n-glo silver wing\",\"rooster tail vibric\",\"startup\",\"technology\",\"775587761, bffulltakes_ftp, arts culture and entertainment\",\"bffulltakes_ftp, 775575301, 20201014_billboard music awards_nbc\",\"775583459, bffulltakes_ftp, arts culture and entertainment\",\"masterlist, tool\",\"logo, font, text\",\"office\",\"spin-n-glo white wing\",\"arts culture and entertainment, 775581420\",\"green\",\"775581421, arts culture and entertainment, bffulltakes_ftp\",\"graphic designer tools\",\"775571579, performance, bffulltakes_ftp\",\"775576796, event, bffulltakes_ftp, arts culture and entertainment\",\"intercontinental san juan resort\",\"775641564, music, fashion, celebrities, awards ceremony, arts culture and entertainment\",\"tool\",\"bffulltakes_ftp, 775574058, arts culture and entertainment\",\"4.1.1 afterpay, font\",\"arts culture and entertainment, 775574058, bffulltakes_ftp\",\"2.1.1 shop now pay later, font\",\"bffulltakes_ftp, 775571574\",\"text, font, line\",\"4.1.2 clearpay, font\",\"bffulltakes_ftp, arts culture and entertainment, 775574058\",\"775571527, bffulltakes_ftp, arts culture and entertainment\",\"775577363, bffulltakes_ftp, arts culture and entertainment\",\"775563042, bffulltakes_ftp\",\"775571579, red, bffulltakes_ftp\",\"line, font, text\",\"drink\",\"bffulltakes_ftp, 775575292\",\"text, font, logo\",\"775581420, red, arts culture and entertainment\",\"bffulltakes_ftp1, 775590397, arts culture and entertainment\",\"20201017_775578202_feedyourcitychallengechicago_516, bffulltakes_ftp\",\"775581418, bffulltakes_ftp, arts culture and entertainment\",\"composite material\",\"775571574, performance, bffulltakes_ftp\",\"bffulltakes_ftp, arts culture and entertainment, 775576796\",\"film\",\"logo, font\",\"arts culture and entertainment, 775590358, bffulltakes_ftp\",\"hilton barbados resort\",\"775578207, bffulltakes_ftp, arts culture and entertainment\",\"20201029_casamigoshalloween_775580960_kovac_488images, bffulltakes_ftp\",\"performance, 775571579, bffulltakes_ftp\"]},\"selected\":{\"id\":\"1132\"},\"selection_policy\":{\"id\":\"1133\"}},\"id\":\"1105\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"LinearScale\"},{\"attributes\":{\"end\":921.5999999999999,\"start\":0},\"id\":\"1078\",\"type\":\"DataRange1d\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"1131\"},\"ticker\":{\"id\":\"1088\"}},\"id\":\"1087\",\"type\":\"LinearAxis\"},{\"attributes\":{\"formatter\":{\"id\":\"1129\"},\"ticker\":{\"id\":\"1085\"}},\"id\":\"1084\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"axis\":{\"id\":\"1084\"},\"grid_line_alpha\":0.5,\"ticker\":null},\"id\":\"1086\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1085\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1097\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"Orange\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1115\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1134\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1135\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1091\",\"type\":\"PanTool\"},{\"attributes\":{\"axis\":{\"id\":\"1087\"},\"dimension\":1,\"ticker\":null},\"id\":\"1090\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1129\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1088\",\"type\":\"BasicTicker\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1091\"},{\"id\":\"1092\"},{\"id\":\"1093\"},{\"id\":\"1094\"},{\"id\":\"1095\"},{\"id\":\"1096\"},{\"id\":\"1111\"}]},\"id\":\"1098\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1092\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1131\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"overlay\":{\"id\":\"1097\"}},\"id\":\"1093\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1094\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1095\",\"type\":\"ResetTool\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"1109\"}],\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"1111\",\"type\":\"HoverTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1113\"},\"glyph\":{\"id\":\"1114\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1115\"},\"selection_glyph\":null,\"view\":{\"id\":\"1117\"}},\"id\":\"1116\",\"type\":\"GlyphRenderer\"}],\"root_ids\":[\"1075\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\n",
       "  var render_items = [{\"docid\":\"3f24b1e8-6a62-4b77-ac27-59b7a1941ff1\",\"root_ids\":[\"1075\"],\"roots\":{\"1075\":\"126dbcb0-833c-46c1-a52b-d29a864cc8cf\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1075"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This next line splits the pairs of <word, count> in the vocabulary into two lists:\n",
    "# 1.  a list of words (types)\n",
    "# 2.  a list of counts (per type)\n",
    "# with the property that the ith word in the list has its corresponding count in the ith counts.\n",
    "words, counts = zip(*vocab.unigram_counts.most_common(vocab.size))\n",
    "counts = np.array(counts, dtype=float)  # Avoid integer math.\n",
    "rank = 1 + np.arange(len(counts))  # rank is an array of [1, 2, 3, 4, ..., num_types]\n",
    "N = np.sum(counts)  # N = total # of tokens seen.\n",
    "p = counts / N  # p is an array the length of `words`.  #_times_word_seen / total_#_words\n",
    "\n",
    "# Fit a power law curve to the histogram above.\n",
    "# Optimize least-squares in log space.\n",
    "# See http://nlp.stanford.edu/IR-book/html/htmledition/zipfs-law-modeling-the-distribution-of-terms-1.html\n",
    "# fit_func = lambda c: (np.log(c[0]*p) - np.log(c[0] * rank**c[1]))\n",
    "fit_func = lambda c: (np.log(p) - np.log(c[0] * rank**c[1]))\n",
    "(a,b), _ = optimize.leastsq(fit_func, np.array([p[0], -1.0]))\n",
    "print(u\"Power law exponent: \\u03B2 = {:.02f}\".format(b))\n",
    "p_pred = (a * rank**b) / sum(a * rank**b)  # predict probabilities\n",
    "c_pred = N * p_pred  # predict counts\n",
    "\n",
    "\n",
    "# Plot counts, with fit curve.\n",
    "nplot = 100\n",
    "fig = bp.figure(x_range=words[:nplot], plot_width=800, plot_height=400)\n",
    "bars = fig.vbar(x=words[:nplot], width=0.8, top=counts[:nplot], hover_fill_color=\"firebrick\")\n",
    "fig.add_tools(HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], renderers=[bars], mode=\"vline\"))\n",
    "fig.line(x=words[:nplot], y=np.round(c_pred)[:nplot], color=\"Orange\", line_width=2)\n",
    "fig.y_range.start = 0\n",
    "fig.y_range.end = 1.2*max(counts)\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "fig.xgrid.grid_line_alpha = 0.5\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should we care about the form of this distribution? Power-law distributions like [Zipf's law](https://en.wikipedia.org/wiki/Zipf%E2%80%93Mandelbrot_law) have a **long tail**, which means that a large fraction of the tokens (words on the page) belong to types (words in the dictionary) that appear quite rarely. \n",
    "\n",
    "We can look at this directly by plotting the cumulative distribution function, *as a function of the count* $c(w)$:\n",
    "\n",
    "$$ f(k) = \\sum_{i \\in \\text{words}}^{|\\text{corpus}|} \\mathbf{1}\\{c(w_i) <= k\\} = \\sum_{j \\in \\text{types}}^{|V|} c(w_j)\\ \\mathbf{1}\\{c(w_j) <= k\\} $$\n",
    "\n",
    "This asks: for a given word on the page, how likely is $c(w) <= k$?\n",
    "\n",
    "The Brown corpus has about 1 million words (tokens), about 7% of which are the token \"the\" and 6% of which are commas. So, we'll plot our data on a log scale so that we can see what's going on for $k= 1, ..., 100$, as well as the higher counts ($c(\\text{the}) = 6.9 \\cdot 10^4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"0ec156d3-f64e-4e5f-b72c-d68d53dd3664\" data-root-id=\"1192\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"f248c2ad-db98-4576-b87f-ea67f5457672\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1203\"}],\"center\":[{\"id\":\"1206\"},{\"id\":\"1210\"}],\"left\":[{\"id\":\"1207\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1228\"},{\"id\":\"1233\"}],\"title\":{\"id\":\"1193\"},\"toolbar\":{\"id\":\"1218\"},\"x_range\":{\"id\":\"1195\"},\"x_scale\":{\"id\":\"1199\"},\"y_range\":{\"id\":\"1197\"},\"y_scale\":{\"id\":\"1201\"}},\"id\":\"1192\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1263\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1264\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"ticker\":null},\"id\":\"1258\",\"type\":\"LogTickFormatter\"},{\"attributes\":{\"axis\":{\"id\":\"1203\"},\"ticker\":null},\"id\":\"1206\",\"type\":\"Grid\"},{\"attributes\":{\"data\":{\"x\":{\"__ndarray__\":\"AAAAAAAA8D8AAAAAAAAAQAAAAAAAAAhAAAAAAAAAEEAAAAAAAAAUQAAAAAAAABhAAAAAAAAAHEAAAAAAAAAgQAAAAAAAACJAAAAAAAAAJEAAAAAAAAAmQAAAAAAAAChAAAAAAAAAKkAAAAAAAAAsQAAAAAAAAC5AAAAAAAAAMEAAAAAAAAAxQAAAAAAAADJAAAAAAAAAM0AAAAAAAAA0QAAAAAAAADVAAAAAAAAANkAAAAAAAAA3QAAAAAAAADhAAAAAAAAAOUAAAAAAAAA6QAAAAAAAADtAAAAAAAAAPEAAAAAAAAA9QAAAAAAAAD5AAAAAAAAAP0AAAAAAAABAQAAAAAAAgEBAAAAAAAAAQUAAAAAAAIBBQAAAAAAAAEJAAAAAAACAQkAAAAAAAABDQAAAAAAAgENAAAAAAAAAREAAAAAAAIBEQAAAAAAAAEVAAAAAAACARUAAAAAAAABGQAAAAAAAgEZAAAAAAAAAR0AAAAAAAIBHQAAAAAAAAEhAAAAAAACASEAAAAAAAIBJQAAAAAAAAEpAAAAAAACASkAAAAAAAIBLQAAAAAAAAExAAAAAAACATEAAAAAAAABNQAAAAAAAAE5AAAAAAACATkAAAAAAAABPQAAAAAAAgE9AAAAAAAAAUEAAAAAAAEBQQAAAAAAAwFBAAAAAAAAAUUAAAAAAAEBRQAAAAAAAwFFAAAAAAAAAUkAAAAAAAEBSQAAAAAAAwFJAAAAAAAAAU0AAAAAAAMBTQAAAAAAAgFRAAAAAAAAAVUAAAAAAAIBVQAAAAAAAwFVAAAAAAACAV0AAAAAAAABYQAAAAAAAQFhAAAAAAACAWEAAAAAAAABZQAAAAAAAwFlAAAAAAAAAW0AAAAAAAEBbQAAAAAAAwF1AAAAAAAAAXkAAAAAAAEBfQAAAAAAAYGBAAAAAAADgYEAAAAAAAABhQAAAAAAAIGJAAAAAAADgYkAAAAAAAABjQAAAAAAAoGNAAAAAAAAAZEAAAAAAAKBkQAAAAAAAYGVAAAAAAABgaEAAAAAAAIBoQAAAAAAAwGlAAAAAAACgakAAAAAAAGBrQAAAAAAAAGxAAAAAAACgbUAAAAAAAMBvQAAAAAAAcHBAAAAAAAAgckAAAAAAAMB0QAAAAAAAAH1AAAAAAAAgfkAAAAAAAJiBQAAAAAAA4IZAAAAAAAAAiEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[112]},\"y\":{\"__ndarray__\":\"cGqR9h3E7T9hwu5AB0buP60e0qnjf+4/sg+itymj7j/cQcar3bnuP9cQRjmWy+4/Oq5zE9Ha7j8QnQ8IXenuP6paYA/d8+4/kkoRVr797j+KjZKPdAfvPzQBEjAIEO8/mO4MUsUV7z9Ok5EfqBvvP5g74WMEIe8/b4YGF5Em7z9SAjH3bCnvP547Zh4MMO8/GExBWKQz7z/O8MUlhznvP4Bnw7ZlPO8/oV8fPY9A7z93tgBnmkbvPxwItUGoTO8/De8nAlZQ7z89qW6YEVPvP4Iv8xboVe8/gPDI1CRZ7z+3tGJDaV3vP7vWLQnvX+8/Ry4dv91i7z+g7/lCjGTvP1VCw2/vZe8/tQ3zyyZp7z8ugSys/WrvP8sfMN5Cbe8/h9+qiW1u7z+ReuNEOXHvP7y7vOWuc+8/vkyiyPF07z9S+coPq3bvP/IotCQzeu8/O4sEP4577z/x3c1r8XzvP5d6aP9AgO8/x4tRxbyA7z/Cu0GgtoLvP17JX++4g+8/+srBxzyE7z9wudE7T4XvP4OdhxFnhu8/W4o1rfWG7z9IT92nHYjvP40sBFa0iO8/ttz3E+eJ7z98gpEzH4vvP38TdxZijO8/z9a8OAaN7z8NU+7eU47vP7PviHKjke8/cKNHp0+S7z+IAGtyrZPvPy+RSY9+lu8/Ic9ef+yX7z/paDwoppjvP7SHQz0kmu8/HZzws6eb7z8oK2349J3vP2sgDJSIn+8/TAtRkSGh7z8pcWwi9qHvP3PHAMbSou8/WxM7y7Sj7z/hVBsynKTvP4vNgWFwpu8/iOX5Tm2n7z8k8xeeb6jvP477CJ50qe8/yP7MTnyq7z+f9zZhiavvPyfK/Kqzre8/xWgA3fiv7z/jMtUmHrHvPxbJ51hesu8/GVrNO6Gz7z8m0dGS8bTvPw0pyA5Stu8/L2wKTr237z8hqh8+K7nvP1m5n2Wxuu8/a6kRske87z9MlFav4L3vPzdluiCHv+8/kCaXpDXB7z/zzZKc8cLvPzBWgLm9xO8/1GE1asrG7z9GaL3L2cjvP886gxUEy+8/AOkNNUHN7z8KeIp5js/vPyDtJTLp0e8/uB545WbU7z8O+MxWEtfvP6qijP/V2e8/G8GMTuLc7z9/BSqgX+DvP5mckB5A5e8/QNbMC1Hq7z9ja8rrO/DvPySTD4ft9+8/AAAAAAAA8D8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[112]}},\"selected\":{\"id\":\"1261\"},\"selection_policy\":{\"id\":\"1262\"}},\"id\":\"1225\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1260\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"start\":0},\"id\":\"1197\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"1225\"}},\"id\":\"1229\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1215\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1227\",\"type\":\"Line\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"white\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1232\",\"type\":\"Circle\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1217\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"text\":\"Cumulative Word Counts\"},\"id\":\"1193\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"1225\"},\"glyph\":{\"id\":\"1226\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1227\"},\"selection_glyph\":null,\"view\":{\"id\":\"1229\"}},\"id\":\"1228\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1195\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1201\",\"type\":\"LinearScale\"},{\"attributes\":{\"overlay\":{\"id\":\"1217\"}},\"id\":\"1213\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"line_color\":\"#1f77b4\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1226\",\"type\":\"Line\"},{\"attributes\":{\"num_minor_ticks\":10},\"id\":\"1204\",\"type\":\"LogTicker\"},{\"attributes\":{},\"id\":\"1262\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"data_source\":{\"id\":\"1230\"},\"glyph\":{\"id\":\"1231\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1232\"},\"selection_glyph\":null,\"view\":{\"id\":\"1234\"}},\"id\":\"1233\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"axis_label\":\"Count (log-scale)\",\"formatter\":{\"id\":\"1258\"},\"ticker\":{\"id\":\"1204\"}},\"id\":\"1203\",\"type\":\"LogAxis\"},{\"attributes\":{},\"id\":\"1199\",\"type\":\"LogScale\"},{\"attributes\":{\"fill_color\":{\"value\":\"white\"},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1231\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1261\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"1228\"}],\"tooltips\":[[\"count\",\"@x\"],[\"CDF\",\"@y\"]]},\"id\":\"1235\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1214\",\"type\":\"SaveTool\"},{\"attributes\":{\"data\":{\"x\":{\"__ndarray__\":\"AAAAAAAA8D8AAAAAAAAAQAAAAAAAAAhAAAAAAAAAEEAAAAAAAAAUQAAAAAAAABhAAAAAAAAAHEAAAAAAAAAgQAAAAAAAACJAAAAAAAAAJEAAAAAAAAAmQAAAAAAAAChAAAAAAAAAKkAAAAAAAAAsQAAAAAAAAC5AAAAAAAAAMEAAAAAAAAAxQAAAAAAAADJAAAAAAAAAM0AAAAAAAAA0QAAAAAAAADVAAAAAAAAANkAAAAAAAAA3QAAAAAAAADhAAAAAAAAAOUAAAAAAAAA6QAAAAAAAADtAAAAAAAAAPEAAAAAAAAA9QAAAAAAAAD5AAAAAAAAAP0AAAAAAAABAQAAAAAAAgEBAAAAAAAAAQUAAAAAAAIBBQAAAAAAAAEJAAAAAAACAQkAAAAAAAABDQAAAAAAAgENAAAAAAAAAREAAAAAAAIBEQAAAAAAAAEVAAAAAAACARUAAAAAAAABGQAAAAAAAgEZAAAAAAAAAR0AAAAAAAIBHQAAAAAAAAEhAAAAAAACASEAAAAAAAIBJQAAAAAAAAEpAAAAAAACASkAAAAAAAIBLQAAAAAAAAExAAAAAAACATEAAAAAAAABNQAAAAAAAAE5AAAAAAACATkAAAAAAAABPQAAAAAAAgE9AAAAAAAAAUEAAAAAAAEBQQAAAAAAAwFBAAAAAAAAAUUAAAAAAAEBRQAAAAAAAwFFAAAAAAAAAUkAAAAAAAEBSQAAAAAAAwFJAAAAAAAAAU0AAAAAAAMBTQAAAAAAAgFRAAAAAAAAAVUAAAAAAAIBVQAAAAAAAwFVAAAAAAACAV0AAAAAAAABYQAAAAAAAQFhAAAAAAACAWEAAAAAAAABZQAAAAAAAwFlAAAAAAAAAW0AAAAAAAEBbQAAAAAAAwF1AAAAAAAAAXkAAAAAAAEBfQAAAAAAAYGBAAAAAAADgYEAAAAAAAABhQAAAAAAAIGJAAAAAAADgYkAAAAAAAABjQAAAAAAAoGNAAAAAAAAAZEAAAAAAAKBkQAAAAAAAYGVAAAAAAABgaEAAAAAAAIBoQAAAAAAAwGlAAAAAAACgakAAAAAAAGBrQAAAAAAAAGxAAAAAAACgbUAAAAAAAMBvQAAAAAAAcHBAAAAAAAAgckAAAAAAAMB0QAAAAAAAAH1AAAAAAAAgfkAAAAAAAJiBQAAAAAAA4IZAAAAAAAAAiEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[112]},\"y\":{\"__ndarray__\":\"cGqR9h3E7T9hwu5AB0buP60e0qnjf+4/sg+itymj7j/cQcar3bnuP9cQRjmWy+4/Oq5zE9Ha7j8QnQ8IXenuP6paYA/d8+4/kkoRVr797j+KjZKPdAfvPzQBEjAIEO8/mO4MUsUV7z9Ok5EfqBvvP5g74WMEIe8/b4YGF5Em7z9SAjH3bCnvP547Zh4MMO8/GExBWKQz7z/O8MUlhznvP4Bnw7ZlPO8/oV8fPY9A7z93tgBnmkbvPxwItUGoTO8/De8nAlZQ7z89qW6YEVPvP4Iv8xboVe8/gPDI1CRZ7z+3tGJDaV3vP7vWLQnvX+8/Ry4dv91i7z+g7/lCjGTvP1VCw2/vZe8/tQ3zyyZp7z8ugSys/WrvP8sfMN5Cbe8/h9+qiW1u7z+ReuNEOXHvP7y7vOWuc+8/vkyiyPF07z9S+coPq3bvP/IotCQzeu8/O4sEP4577z/x3c1r8XzvP5d6aP9AgO8/x4tRxbyA7z/Cu0GgtoLvP17JX++4g+8/+srBxzyE7z9wudE7T4XvP4OdhxFnhu8/W4o1rfWG7z9IT92nHYjvP40sBFa0iO8/ttz3E+eJ7z98gpEzH4vvP38TdxZijO8/z9a8OAaN7z8NU+7eU47vP7PviHKjke8/cKNHp0+S7z+IAGtyrZPvPy+RSY9+lu8/Ic9ef+yX7z/paDwoppjvP7SHQz0kmu8/HZzws6eb7z8oK2349J3vP2sgDJSIn+8/TAtRkSGh7z8pcWwi9qHvP3PHAMbSou8/WxM7y7Sj7z/hVBsynKTvP4vNgWFwpu8/iOX5Tm2n7z8k8xeeb6jvP477CJ50qe8/yP7MTnyq7z+f9zZhiavvPyfK/Kqzre8/xWgA3fiv7z/jMtUmHrHvPxbJ51hesu8/GVrNO6Gz7z8m0dGS8bTvPw0pyA5Stu8/L2wKTr237z8hqh8+K7nvP1m5n2Wxuu8/a6kRske87z9MlFav4L3vPzdluiCHv+8/kCaXpDXB7z/zzZKc8cLvPzBWgLm9xO8/1GE1asrG7z9GaL3L2cjvP886gxUEy+8/AOkNNUHN7z8KeIp5js/vPyDtJTLp0e8/uB545WbU7z8O+MxWEtfvP6qijP/V2e8/G8GMTuLc7z9/BSqgX+DvP5mckB5A5e8/QNbMC1Hq7z9ja8rrO/DvPySTD4ft9+8/AAAAAAAA8D8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[112]}},\"selected\":{\"id\":\"1263\"},\"selection_policy\":{\"id\":\"1264\"}},\"id\":\"1230\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1211\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1208\",\"type\":\"BasicTicker\"},{\"attributes\":{\"source\":{\"id\":\"1230\"}},\"id\":\"1234\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1216\",\"type\":\"HelpTool\"},{\"attributes\":{\"axis_label\":\"p(Count(w) < c)\",\"formatter\":{\"id\":\"1260\"},\"ticker\":{\"id\":\"1208\"}},\"id\":\"1207\",\"type\":\"LinearAxis\"},{\"attributes\":{\"axis\":{\"id\":\"1207\"},\"dimension\":1,\"ticker\":null},\"id\":\"1210\",\"type\":\"Grid\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1211\"},{\"id\":\"1212\"},{\"id\":\"1213\"},{\"id\":\"1214\"},{\"id\":\"1215\"},{\"id\":\"1216\"},{\"id\":\"1235\"}]},\"id\":\"1218\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1212\",\"type\":\"WheelZoomTool\"}],\"root_ids\":[\"1192\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\n",
       "  var render_items = [{\"docid\":\"f248c2ad-db98-4576-b87f-ea67f5457672\",\"root_ids\":[\"1192\"],\"roots\":{\"1192\":\"0ec156d3-f64e-4e5f-b72c-d68d53dd3664\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1192"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll use the histogram function with variable bins in order to get a stair-step plot.\n",
    "b_shift = 0.5  # So counts don't fall on bin boundaries.\n",
    "# Weights give us distribution by token; remove this to get distribution by type.\n",
    "h, bins = np.histogram(counts, weights=counts, bins=b_shift+np.concatenate([[0], np.unique(counts)]))\n",
    "\n",
    "fig = bp.figure(plot_width=800, plot_height=400, x_axis_type=\"log\", title=\"Cumulative Word Counts\")\n",
    "l = fig.line(x=bins[1:]-b_shift, y=np.cumsum(1.0*h)/np.sum(h), line_width=2)\n",
    "fig.circle(x=bins[1:]-b_shift, y=np.cumsum(1.0*h)/np.sum(h), fill_color=\"white\", size=4)\n",
    "fig.add_tools(HoverTool(tooltips=[(\"count\", \"@x\"), (\"CDF\", \"@y\")], renderers=[l], mode=\"vline\"))\n",
    "fig.y_range.start = 0\n",
    "fig.yaxis.axis_label = \"p(Count(w) < c)\"\n",
    "fig.xaxis.axis_label = \"Count (log-scale)\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** the cell above plots the distribution based on *tokens*, or words on the page. Modify the histogram code to make a similar plot, but based on *types*, or distinct words in the vocabulary. If we pick a random word in the dictionary, how many times are we likely to see it in a 1M word corpus?\n",
    "\n",
    "**Exercise (more involved):** compute the same function for the bigram and trigram distributions, and overlay it on a version of the plot above. (This should be similar to a graph from the async.) What does this tell you about the sparsity problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "With a basic idea of what our corpus looks like, we can now embark on the task of modeling it. Recall from the async that language modeling is the task of predicting the next word, given the preceding history:\n",
    "\n",
    "$$ P(w_i | w_{i-1}, ..., w_0)$$\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "How hard is this task? Let's use the unigram distribution as a starting point for our model. We saw that the distribution is very uneven - some words (types) are very common, while others are very rare. If the corpus (or a sample thereof) is our test set, we're going to see our training labels according to the distribution $y \\sim p(w)$. A smarter model might take some features such as immediately previous words and do a better job estimating the next word (let's call such features $x$ in general). We can summarize how uncertain our model is by looking at the entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram entropy: 19.319 bits\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram entropy: {:.03f} bits\".format(stats.entropy(p, base=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bits are still a little unintuitive, but we can do better. Recall from Assignment 1 that the entropy of a _uniform_ distribution over $n$ elements is $ \\log_2 n $. So given some distribution with entropy $H(P) = k$ bits we can say that distribution $P$ is _as uncertain as_ a uniform distribution over $2^k$ elements.\n",
    "\n",
    "If we apply this to our unigram distribution, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct unigrams: 735867\n",
      "As uncertain as:   653866.39\n"
     ]
    }
   ],
   "source": [
    "print(\"Distinct unigrams: {:d}\".format(len(p)))\n",
    "print(\"As uncertain as:   {:.02f}\".format(2**stats.entropy(p, base=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the (real) machine learning setting, we can't really measure the entropy of the underlying distribution $P(y\\ |\\ x) = P(w_i | w_{i-1}, ..., w_0)$. But, we can approximate the cross-entropy between the true distribution (for which we have samples $y_i \\sim P(y\\ |\\ x_i)$) and our _predicted_ distribution $\\hat{P}(y\\ |\\ x_i)$:\n",
    "\n",
    "$$ \\text{CE}(P, \\hat{P}) = - \\sum_{y,x} P(y\\ |\\ x) \\log_2 \\hat{P}(y\\ |\\ x) \\approx - \\frac{1}{N} \\sum_{i}^N \\log_2 \\hat{P}(y_i\\ |\\ x_i)$$\n",
    "\n",
    "This has the same units (bits), and we can exponentiate it in the same way to give us a measure of \"how confused\" the model is. This quantity is the **perplexity** of the model. A perplexity of $k$ tells us that the model is as uncertain as if it had to choose from $k$ elements with equal probability.\n",
    "\n",
    "So suppose we used the unigram model as our language model. We'll cheat for now and just use $\\hat{P}_{\\text{unigram}}(w) = \\tilde{p}(w) $ (where $\\tilde{p}$ denotes an estimate from a finite sample). Then our (training set) perplexity is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram language model\n",
      "Expected full-corpus perplexity: 653866.39\n"
     ]
    }
   ],
   "source": [
    "# scipy.stats.entropy with two arguments\n",
    "def cross_entropy(p, q):\n",
    "    return -1*np.sum(p * np.log2(q))\n",
    "\n",
    "print(\"Unigram language model\")\n",
    "print(\"Expected full-corpus perplexity: {:.02f}\".format(2**cross_entropy(p, q=p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to sanity-check it, we can do our usual machine learning loss calculation: look at all the examples (words), and take the average loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 779402/779402 [00:11<00:00, 65198.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (full-corpus): 653866.39\n"
     ]
    }
   ],
   "source": [
    "word_to_prob = {w:p[i] for i,w in enumerate(words)}\n",
    "probs = [word_to_prob[utils.canonicalize_word(w)] \n",
    "         for w in ProgressBar(wordlist)]\n",
    "perplexity = 2**(-1*np.sum(np.log2(probs))/len(probs))\n",
    "print(\"Perplexity (full-corpus): {:.02f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Language Models\n",
    "\n",
    "The unigram model isn't a very good one - it doesn't model any previous context at all. On the other hand, we can't model _all_ of the preceding words, because that history could get prohibitively long and extremely sparse. As a compromise, we'll make a _Markov assumption_ and limit ourselves to a finite history of $n$ words.\n",
    "\n",
    "For now, we'll build a trigram model, which considers the two preceding words:\n",
    "\n",
    "$$ P(w_i\\ |\\ w_{i-1}, ..., w_0) \\approx P(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n",
    "\n",
    "**Exercise:** how many possible trigrams are there?\n",
    "\n",
    "We'll need to store a table of the probabilities, indexed by triples $(w_i, w_{i-1}, w_{i-2})$. If we store every possible combination, it can get quite large. Assuming 8 bytes for each entry, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:        735,870 words\n",
      "Unigrams need:       5748.98 kB\n",
      "Bigrams  need:    4131352.67 MB\n",
      "Trigrams need:  3040138487800.62 MB\n",
      "Available:          12554.10 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(\"Vocab size:     {:10,} words\".format(vocab.size))\n",
    "print(\"Unigrams need:  {:12.2f} kB\".format(8 * (vocab.size ** 1) / (2**10)))\n",
    "print(\"Bigrams  need:  {:12.2f} MB\".format(8 * (vocab.size ** 2) / (2**20)))\n",
    "print(\"Trigrams need:  {:12.2f} MB\".format(8 * (vocab.size ** 3) / (2**20)))\n",
    "print(\"Available:      {:12.2f} MB\".format(psutil.virtual_memory().available / (2**20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's no good. We can store all possible unigrams, but after that we're toast.\n",
    "\n",
    "Thankfully, we don't have to store everything. Recall that most words only appear a handful of times, and that there are plenty of words in the English language that we don't see in our corpus at all. For bigrams and trigrams, the table will be quite sparse. We need only store the entries that we actually observe; the rest we can take to be zero, or estimate their values on the fly through smoothing or backoff.\n",
    "\n",
    "**Observation:** When building language models, be sure to only keep non-zero counts in your datastructures and assume anything missing is 0.\n",
    "\n",
    "**Exercise:** for a corpus of 1 million words and a vocabulary of 10000, what is the maximum number of bigrams that can be actually _observed_? How about trigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing our Model\n",
    "\n",
    "We'll represent our model with a nested map `context => word => probability`, where word is $w_i$ and for our trigram model, the context is the two preceding words $(w_{i-2}, w_{i-1})$.\n",
    "\n",
    "First, we'll go through the corpus and compute raw trigram counts $c(abc)$, which we'll then normalize into probabilities:\n",
    "\n",
    "$$  P_{abc} = P(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{\\mathrm{c(abc)}}{\\sum_{c'}\\mathrm{c(abc')}} = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n",
    "\n",
    "Here's the code for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize_counter(c):\n",
    "    \"\"\"Given a dictionary of <item, counts>, return <item, fraction>.\"\"\"\n",
    "    total = sum(c.values())\n",
    "    return {w:float(c[w])/total for w in c}\n",
    "\n",
    "class SimpleTrigramLM(object):\n",
    "    def __init__(self, words):\n",
    "        \"\"\"Build our simple trigram model.\"\"\"\n",
    "        # Raw trigram counts over the corpus. \n",
    "        # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n",
    "        self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    \n",
    "        # Iterate through the word stream once.\n",
    "        w_1, w_2 = None, None\n",
    "        for word in words:\n",
    "            if w_1 is not None and w_2 is not None:\n",
    "                # Increment trigram count.\n",
    "                self.counts[(w_2,w_1)][word] += 1\n",
    "            # Shift context along the stream of words.\n",
    "            w_2 = w_1\n",
    "            w_1 = word\n",
    "            \n",
    "        # Normalize so that for each context we have a valid probability\n",
    "        # distribution (i.e. adds up to 1.0) of possible next tokens.\n",
    "        self.probas = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        for context, ctr in self.counts.items():\n",
    "            self.probas[context] = normalize_counter(ctr)\n",
    "            \n",
    "    def next_word_proba(self, word, seq):\n",
    "        \"\"\"Compute p(word | seq)\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        return self.probas[context].get(word, 0.0)\n",
    "    \n",
    "    def predict_next(self, seq):\n",
    "        \"\"\"Sample a word from the conditional distribution.\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        pc = self.probas[context]  # conditional distribution\n",
    "        words, probs = zip(*pc.items())  # convert to list\n",
    "        return np.random.choice(words, p=probs)\n",
    "    \n",
    "    def score_seq(self, seq, verbose=False):\n",
    "        \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n",
    "        score = 0.0\n",
    "        count = 0\n",
    "        # Start at third word, since we need a full context.\n",
    "        for i in range(2, len(seq)):\n",
    "            if (seq[i] == \"<s>\" or seq[i] == \"</s>\"):\n",
    "                continue  # Don't count special tokens in score.\n",
    "            s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n",
    "            score += s\n",
    "            count += 1\n",
    "            # DEBUG\n",
    "            if verbose:\n",
    "                print(\"log P({:s} | {:s}) = {.03f}\".format(seq[i], \" \".join(seq[i-2:i]), s))\n",
    "        return score, count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's train our model. We'll do a proper train-test split this time, so we can evaluate on unseen data. This means that we also have to fix our vocabulary based on the *training* set - which means that some unseen words in the test set will get replaced by `<unk>`. The `Vocabulary` helper class will take care of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test.values)\n",
    "# sent_lite = test.values[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-dc6c8d325c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_test_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/w266-final-project/utils.py\u001b[0m in \u001b[0;36mget_train_test_sents\u001b[0;34m(corpus, split, shuffle)\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0msplits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded {:,} sentences ({:g} tokens)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sents'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 779,402 sentences (1.43624e+08 tokens)\n",
      "Training set: 623,521 sentences (9,230,089 tokens)\n",
      "Test set: 155,881 sentences (2,306,110 tokens)\n"
     ]
    }
   ],
   "source": [
    "split=0.8\n",
    "sentences = np.array(list(wordlist), dtype=object)\n",
    "fmt = (len(sentences), sum(map(len, sentences)))\n",
    "print(\"Loaded {:,} sentences ({:g} tokens)\".format(*fmt))\n",
    "\n",
    "\n",
    "rng = np.random.RandomState()\n",
    "rng.shuffle(sentences)  # in-place\n",
    "split_idx = int(split * len(sentences))\n",
    "train_sents = sentences[:split_idx]\n",
    "test_sents = sentences[split_idx:]\n",
    "\n",
    "for l in range(len(train_sents)):\n",
    "    train_sents[l] = train_sents[l].split(\", \")\n",
    "for l in range(len(test_sents)):\n",
    "    test_sents[l] = test_sents[l].split(\", \")\n",
    "# train_sents = train_sents.split(\",\")\n",
    "# test_sents = test_sents.split(\",\")\n",
    "\n",
    "fmt = (len(train_sents), sum(map(len, train_sents)))\n",
    "print(\"Training set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "fmt = (len(test_sents), sum(map(len, test_sents)))\n",
    "print(\"Test set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "\n",
    "#     return train_sentences, test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Concert', 'Fashion', 'Celebrities', 'Awards Ceremony', 'Performance Art', 'Performing Arts', '775588770', 'Red', 'Event', 'Singer', 'Music', 'Stage', 'Singing', 'Musician', 'Performance', 'Public Event', 'Entertainment', 'BFfulltakes_FTP', 'Arts Culture and Entertainment']),\n",
       "       list(['Text', 'Font', 'Line', 'BFfulltakes_FTP']),\n",
       "       list(['Fashion', '775585825', 'Fashion Show', 'Street Fashion', 'Runway', 'Fashion Design', 'BFfulltakes_FTP', 'Arts Culture and Entertainment']),\n",
       "       ..., list(['Service']),\n",
       "       list(['775588770', 'Music', 'Stage', 'Purple', 'Fashion', 'Magenta', 'Performance', 'Light', 'Violet', 'Celebrities', 'Awards Ceremony', 'BFfulltakes_FTP', 'Arts Culture and Entertainment']),\n",
       "       list(['775641564', 'Clothing', 'Red', 'Standing', 'Guitar', 'Dress', 'Event', 'Music', 'Concert', 'Fashion', 'Celebrities', 'Music Venue', 'Music Artist', 'Awards Ceremony', 'Musical Instrument', 'Arts Culture and Entertainment', 'Band Plays', 'Light', 'Guitar Accessory', 'Fun', 'Stage', 'Artist', 'Musician', 'Microphone', 'Entertainment', 'Performance Art', 'Performing Arts'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 9230089/9230089 [00:28<00:00, 322533.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set vocabulary: 39597 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = vocabulary.Vocabulary(canonicalize_word(w) for w in ProgressBar(utils.flatten(train_sents)))\n",
    "print(\"Train set vocabulary: %d words\" % vocab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on Preprocessing\n",
    "\n",
    "We didn't do this for the unigram LM, but when modeling sentences it's helpful to add special beginning-of-sentence (`<s>`) and end-of-sentence (`</s>`) tokens. \n",
    "\n",
    "This lets the model estimate the probability of a word appearing at the beginning of a sentence, and lets it model the end of a sentence properly, since periods or other punctuation aren't always an accurate guide (e.g. `\"Dr.\"` or `\"Yahoo!\"`).\n",
    "\n",
    "Our padded sentences will look like this:\n",
    "```\n",
    "<s> <s> the cat sat in the hat . </s>\n",
    "```\n",
    "**Exercise:** why do we add two `<s>` tokens at the beginning? (*Hint: this is specific to our trigram model.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 11100652/11100652 [00:10<00:00, 1017866.72it/s]\n",
      "100%|ââââââââââ| 2773753/2773753 [00:02<00:00, 1001954.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM...\n",
      "done in 20.91 s\n"
     ]
    }
   ],
   "source": [
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in ProgressBar(utils.flatten(padded_sentences))], dtype=object)\n",
    "\n",
    "train_tokens = sents_to_tokens(train_sents)\n",
    "test_tokens = sents_to_tokens(test_sents)\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Building trigram LM...\",)\n",
    "lm = SimpleTrigramLM(train_tokens)\n",
    "print(\"done in %.02f s\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2773753"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences\n",
    "\n",
    "Before we run scoring, let's look at some generated sentences from our model. We'll generate them sequentially, one token at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> DGDGDGDGDGDGDGDGDG musical ensemble rock concert song singer pop music music stage performance </s>\n",
      "[9 tokens; log P(seq): -35.21]\n",
      "\n",
      "<s> <s> ss21 spring summer 2021 jakarta fashion week fashionlink eureka </s>\n",
      "[5 tokens; log P(seq): -26.13]\n",
      "\n",
      "<s> <s> realness female fitness high intensity training work out wellness hiit keep fit train female rock concert music stage artist band plays guitarist cymbal folk instrument bassist drummer\n",
      "[20 tokens; log P(seq): -85.19]\n",
      "\n",
      "<s> <s> fashion premiere strapless dress waist dress thigh artist purple musician bffulltakes_ftp performance art fashion awards ceremony arts culture and entertainment </s>\n",
      "[14 tokens; log P(seq): -61.81]\n",
      "\n",
      "<s> <s> music bfselects_ftp DGDGDGDGDGDGDGDGDG city vehicle automotive lighting </s>\n",
      "[6 tokens; log P(seq): -35.14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(lm.predict_next(seq))\n",
    "        # Stop at end-of-sentence\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print(\" \".join(seq))\n",
    "    print(\"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*lm.score_seq(seq)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring our model\n",
    "\n",
    "We'll score our model by running the `score_seq` function, which computes \n",
    "\n",
    "$$ \\text{CE}_{\\text{total}}(y, \\hat{y}) = \\sum_{i=2}^N \\log_2 \\hat{p}(w_i\\ |\\ w_{i-1}, w_{i-2}) $$\n",
    "\n",
    "This is the cross-entropy loss, which is equal to $-1$ times the log-likelihood of the data under our model. As usual, we'll exponentiate to get the perplexity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 21.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-5e1b63fcb65f>:51: RuntimeWarning: divide by zero encountered in log2\n",
      "  s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = lm.score_seq(train_tokens)\n",
    "print(\"Train perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))\n",
    "\n",
    "log_p_data, num_real_tokens = lm.score_seq(test_tokens)\n",
    "print(\"Test perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? Our model gets an absurdly low perplexity on the training data, but a perplexity of _infinity_ on the test data.\n",
    "\n",
    "**Answer:** the n-gram model badly overfits without any smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing and Handling the Unknown\n",
    "\n",
    "Our simple model doesn't have any real mechanism for handling unknown words - if we feed something unseen to `score_seq`, it will assign it a probability of zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-5e1b63fcb65f>:51: RuntimeWarning: divide by zero encountered in log2\n",
      "  s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score_seq([\"<s>\", \"i\", \"love\", \"w266\", \"</s>\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cause of the infinite perplexity, above: $\\log_2 0 = -\\infty $. \n",
    "\n",
    "**Exercise:** besides unknown words, when else would the un-smoothed trigram model predict $p(w\\ |\\ w_{i-1}, w_{i-2}) = 0$ ?\n",
    "\n",
    "Is assuming zero probabilities realistic? Let's look back at our unigram distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% words seen only once: 93.02%\n"
     ]
    }
   ],
   "source": [
    "print(\"% words seen only once: {:.02%}\".format(sum(counts * (counts == 1)) / sum(counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 1 in 50 words were seen only once in our corpus, so we might expect a comparable fraction of words in a new sample to also be previously-unseen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% <unk> in test set: 0.11%\n"
     ]
    }
   ],
   "source": [
    "print(\"% <unk> in test set: {:.02%}\".format(np.sum(np.array(test_tokens) == \"<unk>\") / len(test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use our language model in the wild, we'll need to implement some kind of smoothing to hedge our bets whenever these come up. This will be a major focus of Assignment 3, in which you'll build on the `SimpleTrigramLM` by implementing Laplace (add-k) and Kneser-Ney smoothing."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
