{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAG CORPUS CONSOLIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/emma_tebbe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/emma_tebbe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/emma_tebbe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk.data import find\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import os, sys, time\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import logging\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from scipy import stats, optimize\n",
    "from utils import util, vocabulary\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "util.require_package(\"tqdm\")  # for nice progress bars\n",
    "from tqdm import tqdm as ProgressBar\n",
    "\n",
    "# # Bokeh for plotting.\n",
    "util.require_package(\"bokeh\")\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emma_tebbe/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: SIMPLE EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:   \n",
    "use loss and test function to fine-tune embeddings \n",
    "add preprocessing notebook or keep it separate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define functions to retrieve tag data, preprocess the results, find similar embeddings, and introduce a functional suggestion test (inspired by masked-language-model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_start = time.time()\n",
    "\n",
    "lemma_list = []\n",
    "word_list = []\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def word_preprocessing(word):\n",
    "    lower = word.lower()\n",
    "    punct_replacer = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    rem_punct = lower.translate(punct_replacer)\n",
    "    lemma = [lemmatizer.lemmatize(w) for w in nltk.word_tokenize(rem_punct)]\n",
    "    rem_stop = [w for w in lemma if not w in stop_words]\n",
    "    rem_digits = [re.sub('\\d', '<dig>', i) for i in rem_stop]\n",
    "    lemma_list.append(rem_digits)\n",
    "    word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_matches(model, test_list):\n",
    "    \"\"\"\n",
    "    Given an embedding model and list of tags, gets most similar results based on Word2Vec embeddings \n",
    "    (model constructed in w2v_model.py).\n",
    "    Runs on one row (asset) at a time.\n",
    "    \"\"\"\n",
    "#     ref_list = []\n",
    "    matches = {}\n",
    "#     not_found = 0\n",
    "    for lstring in test_list:\n",
    "        tagset = []\n",
    "        try:\n",
    "            match = model.wv.most_similar(lstring)\n",
    "    #         print(ls, ' : ', match)\n",
    "#             ref_list.append(lstring)\n",
    "            for tag in range(len(match)):\n",
    "                tagset.append(match[tag][0])\n",
    "\n",
    "#             ref_list.append(tagset)\n",
    "            matches[lstring] = tagset\n",
    "        except KeyError:\n",
    "    #         print(ls, ' : ','NOT_FOUND')\n",
    "            pass\n",
    "#     print(\"Not found\", not_found)\n",
    "    return matches\n",
    "\n",
    "# search_list = ['blue']\n",
    "# get_top_matches(model, search_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_prediction(test_dictionary):\n",
    "    \"\"\"\n",
    "    Selects one random key from dictionary and determines if any values for that key\n",
    "    match any other keys in the dictionary (in other words, whether the model's\n",
    "    suggestion for a given tag matches any existing tags for the same asset).\n",
    "    \"\"\"\n",
    "    \n",
    "    rand = random.randint(0, len(test_dictionary) - 1)\n",
    "    keylist = list(test_dictionary.keys())\n",
    "    key = keylist[rand]\n",
    "    suggestions = test_dictionary[key]\n",
    "#     print(key, sugestions)\n",
    "    matches = 0\n",
    "    for suggestion in suggestions:\n",
    "        for key in keylist:\n",
    "            if suggestion == key:\n",
    "#                 print(\"MATCH!\", suggestion,  key)\n",
    "                return(\"MATCH\")\n",
    "    return(\"NO MATCH\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get embedding model and compute loss (for use in hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "loss: 4365436.5\n"
     ]
    }
   ],
   "source": [
    "#get embedding model and compute loss (for use in hyperparameter tuning)\n",
    "for i in range(1,2):\n",
    "    print('epoch:', i)\n",
    "    start = time.time()\n",
    "    epochs = i\n",
    "    vec_size = 10\n",
    "    window = 5\n",
    "    vec_model = w2v_model.retrieve_model_no_id(epochs, vec_size, window)\n",
    "        \n",
    "#     vec_model = models.Word2Vec(test_df.values.tolist(), vector_size=vec_size, window=window, min_count=1, workers=4, compute_loss = True, epochs = epochs)\n",
    "#     end = time.time()\n",
    "    loss = vec_model.get_latest_training_loss()\n",
    "    # perplexity = 2**loss\n",
    "    print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the set of tags for each asset. For each of those tags, we then get  a list of the most similar tags based on the W2V model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 7.919224739074707\n"
     ]
    }
   ],
   "source": [
    "##to do: Make this a function with a parameter for each type of model\n",
    "\n",
    "# get lemmatized tag df with 1 row per asset and each tag in a separate column, covert to list of lists\n",
    "test_df = w2v_model.retrieve_expanded_query()\n",
    "# test_df = df_for_model\n",
    "test_vals = test_df.values[0:1000]\n",
    "# test_vals = test_df[0:1000]\n",
    "\n",
    "\n",
    "#use top_matches method to create a dictionary of related tags suggested by embedding model\n",
    "asset_dicts = []\n",
    "start = time.time()\n",
    "for i in range(len(test_vals)):\n",
    "#     print(\"remaining:\", len(test_vals) - i)\n",
    "    test_list = test_vals[i][test_vals[i] != None]\n",
    "#     rate.append(test_list)\n",
    "# #     print(test_list)\n",
    "    top_matches = get_top_matches(vec_model, test_list)\n",
    "    asset_dicts.append(top_matches)\n",
    "# #     str(test_list)\n",
    "end = time.time()\n",
    "print(\"elapsed:\", end - start)\n",
    "# asset_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of dictionaries where each key is a tag for that asset and each set of values is a list of potential suggestions based on the W2V embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asset_dicts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asset_dicts[0].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional Suggestion Test    \n",
    "We test the effectiveness of this suggestion set by selecting a random tag from each asset and seeing if it matches any other tag assigned to that asset. In other words, if one key matches one of another key's values.    \n",
    "    \n",
    "to do: (Consider averaging this over a few iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate of valid suggestions: 0.753\n"
     ]
    }
   ],
   "source": [
    "#use valid_prediction method to determine useful suggestions\n",
    "asset_results = []\n",
    "for i in asset_dicts:\n",
    "#     print(valid_prediction(i), i.keys())\n",
    "    asset_results.append(valid_prediction(i))\n",
    "print(\"rate of valid suggestions:\", asset_results.count(\"MATCH\")/len(asset_results))\n",
    "# asset_results.count(\"MATCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: LANGUAGE MODEL    \n",
    "     \n",
    "code credit: https://github.com/datasci-w266/2021-summer-main/tree/master/materials/simple_lm   \n",
    "\n",
    "We use a simple trigram model to see if that offers increased suggestion quality, on the assumption that tags will frequently in close context with simialr tags (ie, attached to the same asset).\n",
    "\n",
    "In reality, we found a much lower rate of useful suggestions (0.45) as compared to the simple W2V embedding model (0.75). We attempted to improve our trigram model by alphabetizing each tag list prior to training, in the hope that this would further emphasize relationships between related tags. This approach yielded an even lower valid suggestion rate (0.05). {why?}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_query = w2v_model.lm_retrieve_query()\n",
    "wordlist = get_query['cn'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_counter(c):\n",
    "    \"\"\"Given a dictionary of <item, counts>, return <item, fraction>.\"\"\"\n",
    "    total = sum(c.values())\n",
    "    return {w:float(c[w])/total for w in c}\n",
    "\n",
    "class SimpleTrigramLM(object):\n",
    "    def __init__(self, words):\n",
    "        \"\"\"Build our simple trigram model.\"\"\"\n",
    "        # Raw trigram counts over the corpus. \n",
    "        # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n",
    "        self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    \n",
    "        # Iterate through the word stream once.\n",
    "        w_1, w_2 = None, None\n",
    "        for word in words:\n",
    "            if w_1 is not None and w_2 is not None:\n",
    "                # Increment trigram count.\n",
    "                self.counts[(w_2,w_1)][word] += 1\n",
    "            # Shift context along the stream of words.\n",
    "            w_2 = w_1\n",
    "            w_1 = word\n",
    "            \n",
    "        # Normalize so that for each context we have a valid probability\n",
    "        # distribution (i.e. adds up to 1.0) of possible next tokens.\n",
    "        self.probas = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        for context, ctr in self.counts.items():\n",
    "            self.probas[context] = normalize_counter(ctr)\n",
    "            \n",
    "    def next_word_proba(self, word, seq):\n",
    "        \"\"\"Compute p(word | seq)\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        return self.probas[context].get(word, 0.0)\n",
    "    \n",
    "    def predict_next(self, seq):\n",
    "        \"\"\"Sample a word from the conditional distribution.\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        pc = self.probas[context]  # conditional distribution\n",
    "        words, probs = zip(*pc.items())  # convert to list\n",
    "        return np.random.choice(words, p=probs)\n",
    "    \n",
    "    def score_seq(self, seq, verbose=False):\n",
    "        \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n",
    "        score = 0.0\n",
    "        count = 0\n",
    "        # Start at third word, since we need a full context.\n",
    "        for i in range(2, len(seq)):\n",
    "            if (seq[i] == \"<s>\" or seq[i] == \"</s>\"):\n",
    "                continue  # Don't count special tokens in score.\n",
    "            s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n",
    "            score += s\n",
    "            count += 1\n",
    "            # DEBUG\n",
    "            if verbose:\n",
    "                print(\"log P({:s} | {:s}) = {.03f}\".format(seq[i], \" \".join(seq[i-2:i]), s))\n",
    "        return score, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_query = w2v_model.lm_retrieve_query()\n",
    "# get_query = w2v_model.retrieve_query()\n",
    "\n",
    "wordlist = get_query['cn'].tolist()\n",
    "# wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphabetized version\n",
    "import pandas as pd\n",
    "alpha_get_query = get_query[0:1000]\n",
    "v = np.sort(alpha_get_query.cn.str.split(',', expand=True).fillna(''), axis=1)\n",
    "df = pd.DataFrame(v).agg(','.join, 1).str.strip(',').str.lstrip()\n",
    "\n",
    "# wordlist = df.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 779,402 sentences (1.43624e+08 tokens)\n",
      "Training set: 623,521 sentences (9,233,085 tokens)\n",
      "Test set: 155,881 sentences (2,303,114 tokens)\n"
     ]
    }
   ],
   "source": [
    "split=0.8\n",
    "sentences = np.array(list(wordlist), dtype=object)\n",
    "fmt = (len(sentences), sum(map(len, sentences)))\n",
    "print(\"Loaded {:,} sentences ({:g} tokens)\".format(*fmt))\n",
    "\n",
    "\n",
    "rng = np.random.RandomState()\n",
    "rng.shuffle(sentences)  # in-place\n",
    "split_idx = int(split * len(sentences))\n",
    "train_sents = sentences[:split_idx]\n",
    "test_sents = sentences[split_idx:]\n",
    "\n",
    "for l in range(len(train_sents)):\n",
    "    train_sents[l] = train_sents[l].split(\", \")\n",
    "for l in range(len(test_sents)):\n",
    "    test_sents[l] = test_sents[l].split(\", \")\n",
    "# train_sents = train_sents.split(\",\")\n",
    "# test_sents = test_sents.split(\",\")\n",
    "\n",
    "fmt = (len(train_sents), sum(map(len, train_sents)))\n",
    "print(\"Training set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "fmt = (len(test_sents), sum(map(len, test_sents)))\n",
    "print(\"Test set: {:,} sentences ({:,} tokens)\".format(*fmt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9233085/9233085 [00:28<00:00, 322402.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set vocabulary: 39560 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = vocabulary.Vocabulary(canonicalize_word(w) for w in ProgressBar(util.flatten(train_sents)))\n",
    "print(\"Train set vocabulary: %d words\" % vocab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11103648/11103648 [00:10<00:00, 1043863.24it/s]\n",
      "100%|██████████| 2770757/2770757 [00:02<00:00, 1031578.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM...\n",
      "done in 23.82 s\n"
     ]
    }
   ],
   "source": [
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([util.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in ProgressBar(util.flatten(padded_sentences))], dtype=object)\n",
    "\n",
    "train_tokens = sents_to_tokens(train_sents)\n",
    "test_tokens = sents_to_tokens(test_sents)\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Building trigram LM...\",)\n",
    "lm = SimpleTrigramLM(train_tokens)\n",
    "print(\"done in %.02f s\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<s>', '<s>',\n",
       "       '775662224_north_carolinians_call_on_president_biden',\n",
       "       'DGDGDGDGDGDGDGDGDG', 'font', 'arts culture and entertainment',\n",
       "       't shirt', 'fashion', 'bffulltakes_ftp', '</s>', '<s>', '<s>',\n",
       "       'spring summer 2021', 'mandiri private',\n",
       "       'jakarta fashion week 2021', 'tribute to barli asmara', 'ss21',\n",
       "       'jakarta fashion week', 'barli asmara', 'runway', 'fashion',\n",
       "       '</s>', '<s>', '<s>', 'white', 'line', 'text', '</s>', '<s>',\n",
       "       '<s>', 'DGDGDGDGDGDGDGDGDG', 'art', 'technology', 'purple',\n",
       "       'fashion', 'magenta', 'performing arts',\n",
       "       'arts culture and entertainment', 'concert', 'celebrities',\n",
       "       'entertainment', 'awards ceremony', 'electric blue',\n",
       "       'visual effect lighting', 'sky', 'pink', 'violet', 'plant', 'tree',\n",
       "       'fun', 'music', '</s>', '<s>', '<s>', 'nbcu photo bank',\n",
       "       'season 78', 'DGDGDGDGDGDGDGDGDG', 'flooring', 'font', 'DGDGDGDG',\n",
       "       'nup_193340', 'indoor', 'color', 'awards show', '2020s',\n",
       "       'air date 02/28/2021', 'hat', 'red carpet', 'event',\n",
       "       'fashion design', '</s>', '<s>', '<s>', 'DGDGDGDGDGDGDGDGDG',\n",
       "       'film industry', 'circa.grand.opening_emiller_775581551',\n",
       "       'display device', 'convention', 'bffulltakes_ftp',\n",
       "       'arts culture and entertainment', 'celebrities',\n",
       "       '09.05.2020_775554961 starz power book2 premiere_619 images',\n",
       "       'games', '</s>', '<s>', '<s>', 'DGDGDGDGDGDGDGDGDG',\n",
       "       'tom tom drum', 'string instrument', 'band plays', 'drum',\n",
       "       'string instrument accessory', 'musical instrument accessory',\n",
       "       'folk instrument', 'drumhead', 'guitar', 'music', 'concert',\n",
       "       'entertainment', 'musical instrument', 'singing', 'musician',\n",
       "       'microphone', 'music artist', 'performing arts',\n",
       "       'arts culture and entertainment', 'alan jackson',\n",
       "       'guitar accessory', 'pop music', 'audio equipment',\n",
       "       'musical ensemble', 'idiophone', 'public address system',\n",
       "       'plucked string instruments', 'curtain', 'percussion', 'guitarist',\n",
       "       'electronic instrument', 'artist', '</s>', '<s>', '<s>',\n",
       "       'DGDGDGDGDGDGDGDGDG', 'film industry',\n",
       "       'arts culture and entertainment', 'collar', 'denim',\n",
       "       'street fashion', 'electric blue', 'artist', 'celebrities',\n",
       "       'awards ceremony', 'bfselects_ftp', 'blazer', 'sunglasses',\n",
       "       'standing', 'dress shirt', 'formal wear', 'jeans', 'music',\n",
       "       'sleeve', 'music artist', 'entertainment', 'fashion design',\n",
       "       'performing arts', '</s>', '<s>', '<s>', 'magenta', 'microphone',\n",
       "       'performance', 'music artist', 'bffulltakes_ftp',\n",
       "       'performing arts', 'public address system', 'cap',\n",
       "       'audio equipment', 'electronic device', 'purple', 'public event',\n",
       "       'entertainment', 'stage equipment', 'song', 'violet', '</s>',\n",
       "       '<s>', '<s>', 'DGDGDGDGDGDGDGDGDG',\n",
       "       '20210514_starz_runtheworld_kovac', 'pink', 'plant',\n",
       "       'bffulltakes_ftp', 'arts culture and entertainment', 'sleeve',\n",
       "       'red', 'happy', 'font', 'smile', '</s>', '<s>', '<s>',\n",
       "       'arts culture and entertainment', 'organ', 'jheri curl', 'eyebrow',\n",
       "       'photograph', 'gesture', 'purple', 'celebrities',\n",
       "       'bffulltakes_ftp', 'DGDGDGDGDGDGDGDGDG', 'eyewear', 'ringlet',\n",
       "       'eyelash', 'fun', '</s>', '<s>', '<s>', 'event', 'stage'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0:200]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sample Predictions    \n",
    "When we task our model with generating predictions, we do see some relevance in the results. We quantify this later using our Functional Suggestion Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['green',\n",
       " 'wedding dress',\n",
       " 'fashion',\n",
       " 'shoulder',\n",
       " 'arts culture and entertainment',\n",
       " 'grass',\n",
       " 'gown',\n",
       " 'beauty',\n",
       " 'tableware',\n",
       " 'interaction',\n",
       " 'bride',\n",
       " 'performance',\n",
       " 'wedding ceremony supply']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lm_predictions(l_model, tag, max_length):\n",
    "        seq = [\"<s>\", tag]\n",
    "        for i in range(max_length):\n",
    "            try:\n",
    "                seq.append(l_model.predict_next(seq))\n",
    "            except ValueError:\n",
    "                seq.append('nodata_nodata')\n",
    "                \n",
    "        ## dedupe list of suggested tags\n",
    "        seq = set(seq)\n",
    "        seq = list(seq)\n",
    "        seq = [i for i in seq if i not in ['<s>','</s>', tag, 'nodata_nodata']]\n",
    "        \n",
    "        ## n prevents an infinite loop in the next section\n",
    "        n=0\n",
    "        \n",
    "        ## take length of deduped list and use it to return 15 suggestions\n",
    "        while len(seq) < max_length+2 and n < 50:\n",
    "            try:\n",
    "                seq.append(l_model.predict_next(seq))\n",
    "            except ValueError:\n",
    "                seq.append('nodata_nodata')\n",
    "            seq = set(seq)\n",
    "            seq = list(seq)\n",
    "            seq = [i for i in seq if i not in ['<s>','</s>', tag, 'nodata_nodata']]\n",
    "            n+=1\n",
    "        seq = seq[2:]\n",
    "        return seq\n",
    "\n",
    "lm_predictions(lm, 'blue', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring  \n",
    "We check the perplexity and then employ the same functional suggestion test that we used for the W2V embeddings.    \n",
    "We see that the rate of valid predictions for the LM is actually much lower than that of the simple embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 21.65\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = lm.score_seq(train_tokens)\n",
    "print(\"Train perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.976"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for i in asset_dicts:\n",
    "    lengths.append(len(i.values()))\n",
    "np.average(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_get_top_matches(l_model, test_list):\n",
    "    \"\"\"\n",
    "    Given a language model and list of tags, gets most similar results.\n",
    "    Runs on one row (asset) at a time.\n",
    "    \"\"\"\n",
    "#     ref_list = []\n",
    "    matches = {}\n",
    "#     not_found = 0\n",
    "    for lstring in test_list:\n",
    "        tagset = []\n",
    "        try:\n",
    "            match = lm_predictions(lm, lstring, 15)\n",
    "            for tag in range(len(match)):\n",
    "#             for tag in range(2):\n",
    "                tagset.append(match[tag])\n",
    "\n",
    "# #             ref_list.append(tagset)\n",
    "            matches[lstring] = tagset\n",
    "        except KeyError:\n",
    "    #         print(ls, ' : ','NOT_FOUND')\n",
    "            pass\n",
    "#     print(\"Not found\", not_found)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 35.433486223220825\n"
     ]
    }
   ],
   "source": [
    "##to do: Make this a function with a parameter for each type of model\n",
    "\n",
    "# get lemmatized tag df with 1 row per asset and each tag in a separate column, covert to list of lists\n",
    "test_df = w2v_model.retrieve_expanded_query()\n",
    "lm_test_vals = test_df.values[0:100]\n",
    "  \n",
    "#use top_matches method to create a dictionary of related tags suggested by embedding model\n",
    "lm_asset_dicts = []\n",
    "start = time.time()\n",
    "for i in range(len(lm_test_vals)):\n",
    "#     print(\"remaining:\", len(test_vals) - i)\n",
    "    lm_test_list = lm_test_vals[i][lm_test_vals[i] != None]\n",
    "#     rate.append(test_list)\n",
    "# #     print(test_list)\n",
    "    lm_top_matches = lm_get_top_matches(lm, lm_test_list)\n",
    "    lm_asset_dicts.append(lm_top_matches)\n",
    "# #     str(test_list)\n",
    "end = time.time()\n",
    "print(\"elapsed:\", end - start)\n",
    "# lm_asset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of valid suggestions: 0.41\n"
     ]
    }
   ],
   "source": [
    "#use valid_prediction method to determine useful suggestions\n",
    "lm_asset_results = []\n",
    "for i in lm_asset_dicts:\n",
    "#     print(valid_prediction(i), i.keys())\n",
    "    lm_asset_results.append(valid_prediction(i))\n",
    "print(\"Rate of valid suggestions:\", lm_asset_results.count(\"MATCH\")/len(lm_asset_results))\n",
    "# asset_results.count(\"MATCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3. BERT ATTEMPT  \n",
    "Given that our corpus is full of unusual terms and that our \"sentences\" are order-agnostic, BERT's pre-trained bi-directional nature makes it a counterintuitive choice. We propose a novel application, however, in which BERT is fine-tuned on our tag corpus. As in our LM test, we order tags in our corpus alphabetically to impose a sense of word order significance.\n",
    "        \n",
    "BERT plan:    \n",
    "1. create version of reconstructed_assets with rare tags removed, each set of tags alphabetized   \n",
    "2. fine-tune BERT on that <<---- this is where I'm stuck :( \n",
    "3. give BERT an unedited tag list for a given asset, with rare tags changed to [MASK]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code credit: https://gist.github.com/yuchenlin/a2f42d3c4378ed7b83de65c7a2222eb2\n",
    "# !pip install torchvision \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "def predict_masked_sent(text, top_k=5):\n",
    "    # Tokenize input\n",
    "    text = \"[CLS] %s [SEP]\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    masked_index = tokenized_text.index(\"[MASK]\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "\n",
    "        \n",
    "predict_masked_sent(\"BERT is [MASK] at this.\", top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla BERT given a test tag sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_masked_sent(\"[MASK], 'Lighting', 'Arts Culture and Entertainment', 'Light Fixture', 'Water', 'Chandelier', 'Ceiling Fixture', 'Ceiling', 'BFfulltakes_FTP'\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  TEST WITH FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4. SUGGESTION FUNCTION IMPLEMENTATION   \n",
    "We loop through the list of assets and attempt to offer alternative tags for any rare tags that we enconter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the simple W2V embeddings for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check = w2v_model.lm_retrieve_expanded_query()\n",
    "to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:  \n",
    "add more iterations to feed result list of tags back into function for further refinement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#given a list of tags, identify those that are rare and provide suggested replacements\n",
    "def get_candidates(tag_list):\n",
    "    start = time.time()\n",
    "    lemma_dict = w2v_model.lemma_map()\n",
    "    lemma_common_tags = []\n",
    "    lemma_rare_tags = []\n",
    "    candidates = []\n",
    "#     cand_list = []\n",
    "    for tag in tag_list:\n",
    "        try:\n",
    "            if w2v_model.is_not_rare(lemma_dict[tag]):\n",
    "                lemma_common_tags.append(lemma_dict[tag])\n",
    "                continue\n",
    "            else:\n",
    "                lemma_rare_tags.append(lemma_dict[tag])\n",
    "        except KeyError:\n",
    "            if w2v_model.is_not_rare(tag):\n",
    "                continue\n",
    "        try:    \n",
    "            if tag is not None:\n",
    "                candidate = get_top_matches(vec_model, ['', lemma_dict[tag]])\n",
    "    #             print('tag: ', tag, '\\nsuggestions:', candidates.values(),'\\n')\n",
    "#                 cand_list.append(candidate)\n",
    "                candidates.append(candidate.values())\n",
    "        except KeyError:\n",
    "            pass\n",
    "    flat = list(chain(*candidates))\n",
    "    flatter = list(chain(*flat)) \n",
    "#     print('lemma common tags: ', lemma_common_tags, '\\nlemma rare tags: ', lemma_rare_tags)      \n",
    "    return flatter\n",
    "\n",
    "def suggest_better_tags(list_of_tags):\n",
    "    tag_candidates = get_candidates(list_of_tags)\n",
    "    tag_candidates = [w2v_model.delete_rare(tc) for tc in tag_candidates]\n",
    "    tag_candidates = [tag for tag in tag_candidates if tag != \"\"]\n",
    "    \n",
    "    return tag_candidates\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "# list_of_tags = to_check.values[2]\n",
    "# list_of_tags = [\"Saw\"]\n",
    "# zzz = suggest_better_tags(list_of_tags, 1)\n",
    "# zzz\n",
    "# # print(lemma_dict(list_of_tags), zzz)\n",
    "# # yyy = get_candidates(list_of_tags)\n",
    "# # yyy\n",
    "# list_of_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather suggestions for every tag and concatenate those into a\n",
    "single suggestion list.    \n",
    "Then we discard uncommon (rare) suggestions and identify suggestions that were already tags for the given asset (duplicates).     \n",
    "The final list of suggested tags then consists only of common tags that are not already applied to the asset.   \n",
    "We also track the duplicates so they can be used to validate the usability of our suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each asset (row), gather suggestions for every tag and concatenate those into a\n",
    "single suggestion list. Then discard uncommon (rare) suggestions and identify\n",
    "suggestions that were already tags for the given asset (duplicates). The final list of suggested tags\n",
    "is then only common tags that are not already applied to the asset.\n",
    "\n",
    "This function also tracks the duplicates so they can be used to validate the usability of our suggestions.\n",
    "'''\n",
    "\n",
    "def get_real_suggestions(existing_tags):\n",
    "    #GIVE THIS FUNCTION AT LEAST TWO LISTS OF TAGS OR A LIST WRAPPED IN AN EMPTY LIST\n",
    "    start = time.time()\n",
    "    count = len(existing_tags)\n",
    "    \n",
    "    all_live_suggestions = []\n",
    "    all_new_suggestions = []\n",
    "    all_dupe_suggestions = []\n",
    "    all_weighted = []\n",
    "\n",
    "    lemma_dict = w2v_model.lemma_map()\n",
    "    for i in range(count):\n",
    "        live_tags = existing_tags[i]\n",
    "#         print(\"tags in \\n\", live_tags, '\\n')\n",
    "        live_suggestions = suggest_better_tags(live_tags)\n",
    "        all_live_suggestions.append(live_suggestions)\n",
    "    #     print(\"all suggestions \\n\", live_suggestions, '\\n')\n",
    "        new_suggestions = []\n",
    "        dupe_suggestions = []\n",
    "        for sug in live_suggestions:\n",
    "            for tag in live_tags:\n",
    "                if sug == lemma_dict.get(tag):\n",
    "                    if sug not in dupe_suggestions:\n",
    "                        dupe_suggestions.append(sug)\n",
    "                    continue              \n",
    "            if sug not in dupe_suggestions:\n",
    "                new_suggestions.append(sug)\n",
    "        counts = collections.Counter(new_suggestions)\n",
    "        weighted = counts.most_common()\n",
    "        all_weighted.append(weighted)\n",
    "        \n",
    "        all_dupe_suggestions.append(dupe_suggestions)\n",
    "        all_new_suggestions.append(new_suggestions)\n",
    "#         counts = collections.Counter(all_new_suggestions)\n",
    "#         weigthed = counts.most_common()\n",
    "    return [all_live_suggestions, all_dupe_suggestions, all_new_suggestions, all_weighted]\n",
    "test_check = to_check.values[0:100]\n",
    "\n",
    "start = time.time()\n",
    "real_suggestions = get_real_suggestions(test_check)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function returns a batch of new legitimate suggested tags for each asset (ideally), sorted by frequency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(real_suggestions[0])):\n",
    "    print('\\n\\n Asset', i, '\\n\\nORIGINAL TAG LIST: \\n', test_check[i], '\\nSUGGESTIONS: \\n', real_suggestions[0][i], '\\nDUPLICATES: \\n', real_suggestions[1][i], '\\nLEGITIMATE SUGGESTIONS:  \\n', real_suggestions[2][i],  '\\nBY WEIGHT:  \\n', real_suggestions[3][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: scoring/stats, etc   \n",
    "We see at least one valid, common suggestion (defined as a tag that was actually attached by a user in our initial data set) for approx 80% of assets. (based on 1000 assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = real_suggestions[1]\n",
    "# def condition(x): return len(x) == 0\n",
    "output = [idx for idx, element in enumerate(dupes) if len(element) > 0]\n",
    "len(output)/len(real_suggestions[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function suggests an average of approx 10 common tags per asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unique = real_suggestions[3]\n",
    "new_unique_lengths = []\n",
    "for nu in new_unique:\n",
    "    l = len(nu)\n",
    "    new_unique_lengths.append(l)\n",
    "sum(new_unique_lengths)/len(new_unique_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random list of common tags\n",
    "\n",
    "rand_tags = w2v_model.retrieve_rare(3)\n",
    "rand_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_end = time.time()\n",
    "print(nb_end - nb_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
