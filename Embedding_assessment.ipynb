{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/embedding loss assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do: use loss and test function to fine-tune embeddings, add rare/non-rare consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ejhaselden/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ejhaselden/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ejhaselden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ejhaselden/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ejhaselden/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ejhaselden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def word_preprocessing(word):\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    # from nltk.corpus import wordnet\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    lemma_list = []\n",
    "    word_list = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    lower = word.lower()\n",
    "    punct_replacer = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    rem_punct = lower.translate(punct_replacer)\n",
    "    lemma = [lemmatizer.lemmatize(w) for w in nltk.word_tokenize(rem_punct)]\n",
    "    rem_stop = [w for w in lemma if not w in stop_words]\n",
    "    rem_digits = [re.sub('\\d', '<dig>', i) for i in rem_stop]\n",
    "    lemma_list.append(rem_digits)\n",
    "    word_list.append(word)\n",
    "    \n",
    "    \n",
    "for i in [\"BLue\", \"Red\"]:\n",
    "    word_preprocessing(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_matches(model, test_list):\n",
    "    \"\"\"\n",
    "    Given an embedding model and list of tags, gets most similar results (most_similar method).\n",
    "    To be run on one \n",
    "    \"\"\"\n",
    "#     ref_list = []\n",
    "    matches = {}\n",
    "#     not_found = 0\n",
    "    for lstring in test_list:\n",
    "        tagset = []\n",
    "        try:\n",
    "            match = model.wv.most_similar(lstring)\n",
    "    #         print(ls, ' : ', match)\n",
    "#             ref_list.append(lstring)\n",
    "            for tag in range(len(match)):\n",
    "                tagset.append(match[tag][0])\n",
    "\n",
    "#             ref_list.append(tagset)\n",
    "            matches[lstring] = tagset\n",
    "        except KeyError:\n",
    "    #         print(ls, ' : ','NOT_FOUND')\n",
    "            pass\n",
    "#     print(\"Not found\", not_found)\n",
    "    return matches\n",
    "\n",
    "# search_list = rare\n",
    "# get_top_matches(model, search_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tag(test_dictionary):\n",
    "    \"\"\"\n",
    "    Selects one random key from dictionary and determines if any values for that key\n",
    "    match any other keys in the dictionary (in other words, whether the model's\n",
    "    suggestion for a given tag matches any existing tags for the same asset).\n",
    "    \"\"\"\n",
    "    import random\n",
    "    rand = random.randint(0, len(test_dictionary) - 1)\n",
    "    keylist = list(test_dictionary.keys())\n",
    "    key = keylist[rand]\n",
    "    suggestions = test_dictionary[key]\n",
    "#     print(key, sugestions)\n",
    "    matches = 0\n",
    "    for suggestion in suggestions:\n",
    "        for key in keylist:\n",
    "            if suggestion == key:\n",
    "#                 print(\"MATCH!\", suggestion,  key)\n",
    "                return(\"MATCH\")\n",
    "    return(\"NO MATCH\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "30.961588144302368\n",
      "4410028.5\n"
     ]
    }
   ],
   "source": [
    "#get embedding model and compute loss (for use in hyperparameter tuning)\n",
    "import w2v_model\n",
    "import time\n",
    "\n",
    "for i in range(1,2):\n",
    "    print(i)\n",
    "    start = time.time()\n",
    "    epochs = i\n",
    "    vec_size = 10\n",
    "    window = 5\n",
    "    model = w2v_model.retrieve_model_na(epochs, vec_size, window)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    loss = model.get_latest_training_loss()\n",
    "    # perplexity = 2**loss\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.686035394668579\n"
     ]
    }
   ],
   "source": [
    "# get lemmatized tag df with 1 row per asset and each tag in a separate column, covert to list of lists\n",
    "test = w2v_model.retrieve_expanded_query()\n",
    "test_vals = test.values[1:100]\n",
    "\n",
    "#use top_matches method to create a dictionary of related tags suggested by embedding model\n",
    "asset_dicts = []\n",
    "start = time.time()\n",
    "for i in range(len(test_vals)):\n",
    "#     print(\"remaining:\", len(test_vals) - i)\n",
    "    test_list = test_vals[i][test_vals[i] != None]\n",
    "#     rate.append(test_list)\n",
    "# #     print(test_list)\n",
    "    top_matches = get_top_matches(model, test_list)\n",
    "    asset_dicts.append(top_matches)\n",
    "# #     str(test_list)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "# asset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['<dig><dig><dig><dig><dig><dig><dig><dig><dig>', 'boxing',\n",
       "        'sport equipment', ..., None, None, None],\n",
       "       ['<dig><dig><dig><dig><dig><dig><dig><dig><dig>', 'red', 'night',\n",
       "        ..., None, None, None],\n",
       "       ['backstage',\n",
       "        '<dig><dig><dig><dig><dig><dig><dig><dig><dig> <dig><dig><dig><dig> k love fan award backstage',\n",
       "        'original', ..., None, None, None],\n",
       "       ...,\n",
       "       ['bfselects ftp', '<dig><dig><dig><dig><dig><dig><dig><dig><dig>',\n",
       "        'plastic bottle', ..., None, None, None],\n",
       "       ['<dig><dig><dig><dig><dig><dig><dig><dig><dig>', 'red',\n",
       "        'fashion', ..., None, None, None],\n",
       "       ['film industry', '<dig><dig><dig><dig><dig><dig><dig><dig><dig>',\n",
       "        'led display', ..., None, None, None]], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7302473024730247"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use test_tag method to determine useful suggestions\n",
    "asset_results = []\n",
    "for i in asset_dicts:\n",
    "    asset_results.append(test_tag(i))\n",
    "asset_results.count(\"MATCH\")/len(asset_results)\n",
    "# asset_results.count(\"MATCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:  \n",
    "    use lm1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
